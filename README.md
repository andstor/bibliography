# Bibliography

# Table of contents

* [Todo 1](#todo-1)
  * [Todo 2](#todo-2)

# Todo 1 :dart:

## Todo 1
* (book) [Title](https://todo.html), Surname F. (yyyy).
* **`name`** [Title](http://www.todo.pdf), Surname1 F1. Surname1 F2. (yyyy).
* [Title](http://todo.pdf), Surname1 F1. et al. (yyyy). [:film_strip:](https://www.youtube.com) [:octocat:](https://github.com/)


# Todo 2 :dart:

## Todo 1

# Supervised Learning :label:

## Classification :cat: :dog:

## Regression

# Semi-supervised Learning

# Unsupervised Learning  :person_with_probing_cane:

## Clustering

## Dimentionality Reduction

# Self-supervised Learning

## Todo 2
* **`Codex`** [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374), Chen Mark. et al. (2021).


# Reinforcement Learning :robot:

* [Reinforcement learning: A survey](https://www.jair.org/index.php/jair/article/view/10166/24110), Kaelbling L. et al. (1996).


## Policy-based :muscle:

### Policy gradient
* **`PPO`** [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347), Schulman J. et al. (2017). [üéûÔ∏è](https://www.youtube.com/watch?v=bqdjsmSoSgI)

* **`InstructGPT`** [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155), Ouyang L. et al. (2022). [:black_joker:](https://github.com/openai/following-instructions-human-feedback/blob/main/model-card.md)
